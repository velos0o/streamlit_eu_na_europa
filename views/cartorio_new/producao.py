import streamlit as st
import pandas as pd
from datetime import datetime, time
import numpy as np # Adicionado para c√°lculos de desvio padr√£o
import os # Importar os para manipula√ß√£o de caminhos

# Obter o diret√≥rio do arquivo atual
_PRODUCAO_DIR = os.path.dirname(os.path.abspath(__file__))
# Construir o caminho para a pasta assets subindo dois n√≠veis (views/cartorio_new -> streamlit_eu_na_europa)
_ASSETS_DIR = os.path.join(_PRODUCAO_DIR, '..', '..', 'assets')
_CSS_PATH = os.path.join(_ASSETS_DIR, 'styles', 'css', 'main.css')

# --- Mapeamento e Categoriza√ß√£o de Campos ---

# Mapeia nomes t√©cnicos para nomes leg√≠veis, categorias e origens (se aplic√°vel)
CAMPOS_DATA = {
    'UF_CRM_DATA_CERTIDAO_EMITIDA': {'nome': 'Certid√£o Emitida (Cart√≥rio)', 'categoria': 'SUCESSO'},
    'UF_CRM_DATA_CERTIDAO_FISICA_ENTREGUE': {'nome': 'Certid√£o F√≠sica Recebida (Interno)', 'categoria': 'SUCESSO'},
    'UF_CRM_DATA_CERTIDAO_FISICA_ENVIADA': {'nome': 'Certid√£o F√≠sica Enviada (Cart√≥rio)', 'categoria': 'SUCESSO'},

    'UF_CRM_DATA_AGUARDANDOCARTORIO_ORIGEM': {'nome': 'Solicitado ao Cart. Origem (Cart√≥rio)', 'categoria': 'EM ANDAMENTO'},
    'UF_CRM_DATA_MONTAR_REQUERIMENTO': {'nome': 'Requerimento Assinado (Interno)', 'categoria': 'EM ANDAMENTO'},
    'UF_CRM_DATA_SOLICITAR_CARTORIO_ORIGEM': {'nome': 'Requerimento Montado (Interno)', 'categoria': 'EM ANDAMENTO'},
    'UF_CRM_DATA_SOLICITAR_CARTORIO_ORIGEM_PRIORIDADE': {'nome': 'Requerimento Montado - Prioridade (Interno)', 'categoria': 'EM ANDAMENTO'},
    'UF_CRM_DATA_SOLICITAR_REQUERIMENTO': {'nome': 'Solicita√ß√£o Requerimento (Autom√°tico)', 'categoria': 'EM ANDAMENTO'},

    'UF_CRM_DEVOLUCAO_ADM': {'nome': 'Falha na Montagem Requerimento (Interno)', 'categoria': 'FALHA'},
    'UF_CRM_DEVOLUCAO_ADM_VERIFICADO': {'nome': 'Devolu√ß√£o ADM Verificada (?)', 'categoria': 'FALHA'}, # Aguardando regra
    'UF_CRM_DEVOLUTIVA_BUSCA': {'nome': 'Busca Sem Sucesso', 'categoria': 'FALHA'},
    'UF_CRM_DEVOLUTIVA_REQUERIMENTO': {'nome': 'Requerimento N√£o Aceito (Cart√≥rio)', 'categoria': 'FALHA'},
}

# Campos de respons√°vel (usados para a tabela) - apenas a lista de nomes t√©cnicos
CAMPOS_RESPONSAVEL = [
    'UF_CRM_MONTAGEM_PASTA_RESPONSAVEL',
    'UF_CRM_RESPONSAVEL__BUSCA',
    'UF_CRM_RESPONSAVEL_AGUARDANDO_CARTORIO_DE_ORIGEM',
    'UF_CRM_RESPONSAVEL_ANTES_MOVIMENTACAO',
    'UF_CRM_RESPONSAVEL_CERTIDAO_EMITIDA',
    'UF_CRM_RESPONSAVEL_CERTIDO_FISICA_ENTREGUE',
    'UF_CRM_RESPONSAVEL_CERTIDO_FISICA_ENVIADA',
    'UF_CRM_RESPONSAVEL_DEVOLUCAO_ADM',
    'UF_CRM_RESPONSAVEL_DEVOLUCAO_ADM_VERIFICADO',
    'UF_CRM_RESPONSAVEL_DEVOLUTIVA_BUSCA',
    'UF_CRM_RESPONSAVEL_DEVOLVIDOREQUERIMENTO',
    'UF_CRM_RESPONSAVEL_SOLICITACAO_DUPLICADA',
    'UF_CRM_RESPONSAVEL_SOLICITAR_CARTORIO_ORIGEM',
]

# Mapeia campos de respons√°vel para os campos de data correspondentes (aproximado)
# IMPORTANTE: Este mapeamento pode precisar de ajuste fino!
MAPA_RESP_DATA = {
    'UF_CRM_RESPONSAVEL_CERTIDAO_EMITIDA': 'UF_CRM_DATA_CERTIDAO_EMITIDA',
    'UF_CRM_RESPONSAVEL_CERTIDO_FISICA_ENTREGUE': 'UF_CRM_DATA_CERTIDAO_FISICA_ENTREGUE',
    'UF_CRM_RESPONSAVEL_CERTIDO_FISICA_ENVIADA': 'UF_CRM_DATA_CERTIDAO_FISICA_ENVIADA',
    'UF_CRM_RESPONSAVEL_AGUARDANDO_CARTORIO_DE_ORIGEM': 'UF_CRM_DATA_AGUARDANDOCARTORIO_ORIGEM',
    'UF_CRM_MONTAGEM_PASTA_RESPONSAVEL': 'UF_CRM_DATA_MONTAR_REQUERIMENTO', # Suposi√ß√£o
    'UF_CRM_RESPONSAVEL_SOLICITAR_CARTORIO_ORIGEM': 'UF_CRM_DATA_SOLICITAR_CARTORIO_ORIGEM',
    # 'UF_CRM_RESPONSAVEL_SOLICITAR_CARTORIO_ORIGEM_PRIORIDADE': 'UF_CRM_DATA_SOLICITAR_CARTORIO_ORIGEM_PRIORIDADE', # Campo resp. n√£o listado?
    # 'UF_CRM_RESPONSAVEL_SOLICITAR_REQUERIMENTO': 'UF_CRM_DATA_SOLICITAR_REQUERIMENTO', # Campo resp. n√£o listado?
    'UF_CRM_RESPONSAVEL_DEVOLUCAO_ADM': 'UF_CRM_DEVOLUCAO_ADM',
    'UF_CRM_RESPONSAVEL_DEVOLUCAO_ADM_VERIFICADO': 'UF_CRM_DEVOLUCAO_ADM_VERIFICADO',
    'UF_CRM_RESPONSAVEL_DEVOLUTIVA_BUSCA': 'UF_CRM_DEVOLUTIVA_BUSCA',
    'UF_CRM_RESPONSAVEL_DEVOLVIDOREQUERIMENTO': 'UF_CRM_DEVOLUTIVA_REQUERIMENTO', # Suposi√ß√£o
    # Faltam mapeamentos para: _BUSCA, _ANTES_MOVIMENTACAO, _SOLICITACAO_DUPLICADA
}

def exibir_producao(df_original):
    st.subheader("Produ√ß√£o")

    # --- Carregar CSS Compilado ---
    try:
        # Usar o caminho absoluto calculado
        print(f"[Debug Produ√ß√£o] Tentando carregar CSS de: {_CSS_PATH}")
        if os.path.exists(_CSS_PATH):
            with open(_CSS_PATH, 'r', encoding='utf-8') as f:
                st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
                print("[Debug Produ√ß√£o] CSS carregado com sucesso.")
        else:
            st.warning(f"Arquivo CSS principal n√£o encontrado em: {_CSS_PATH}")
            print(f"[Debug Produ√ß√£o] Falha ao carregar CSS: Arquivo n√£o existe em {_CSS_PATH}")
    except Exception as e:
        st.error(f"Ocorreu um erro inesperado ao carregar o CSS em Produ√ß√£o: {e}")
    # --- Fim Carregar CSS ---

    if df_original.empty:
        st.warning("N√£o h√° dados dispon√≠veis para an√°lise de produ√ß√£o.")
        return

    df = df_original.copy() # Trabalhar com uma c√≥pia

    # --- Verificar Colunas Essenciais e Calcular Primeira Data ---
    colunas_data_necessarias = list(CAMPOS_DATA.keys())
    colunas_resp_necessarias = CAMPOS_RESPONSAVEL
    colunas_presentes = df.columns.tolist()

    colunas_data_faltando = [col for col in colunas_data_necessarias if col not in colunas_presentes]
    colunas_resp_faltando = [col for col in colunas_resp_necessarias if col not in colunas_presentes]

    # Calcular data do primeiro registro ANTES de converter e potencialmente perder dados
    primeira_data_geral = pd.NaT
    datas_validas_geral = []
    for col in colunas_data_necessarias:
        if col in df.columns:
            # Tenta converter para datetime, coerce for√ßa NaT em erros
            datas_col = pd.to_datetime(df[col], errors='coerce')
            # Adiciona apenas datas v√°lidas √† lista
            datas_validas_geral.append(datas_col.dropna())

    if datas_validas_geral:
        # Concatena todas as s√©ries de datas v√°lidas
        todas_as_datas = pd.concat(datas_validas_geral)
        if not todas_as_datas.empty:
            primeira_data_geral = todas_as_datas.min()

    if colunas_data_faltando:
        st.error(f"Erro: As seguintes colunas de DATA est√£o faltando no DataFrame e s√£o necess√°rias para esta an√°lise: {', '.join(colunas_data_faltando)}")
        st.info("Verifique se os nomes dos campos est√£o corretos e se o data_loader.py est√° carregando estas colunas.")
        # return # Poderia parar aqui, mas vamos tentar continuar com o que temos

    if colunas_resp_faltando:
        st.warning(f"Aviso: As seguintes colunas de RESPONS√ÅVEL est√£o faltando e s√£o necess√°rias para a tabela de an√°lise por respons√°vel: {', '.join(colunas_resp_faltando)}")
        # A tabela de respons√°veis pode ficar incompleta ou vazia

    # Exibir data do primeiro registro
    if pd.notna(primeira_data_geral):
        st.caption(f"üóìÔ∏è Primeiro registro de atividade encontrado em: {primeira_data_geral.strftime('%d/%m/%Y')}")
    else:
        st.caption("üóìÔ∏è Nenhuma data de atividade encontrada nos dados.")

    # --- Filtro de Data ---
    col1, col2 = st.columns(2)
    with col1:
        data_inicial = st.date_input("Data Inicial", value=pd.to_datetime(datetime.now().date()) - pd.Timedelta(days=30))
    with col2:
        data_final = st.date_input("Data Final", value=pd.to_datetime(datetime.now().date()))

    # Converter para datetime com hora inicial e final para incluir o dia todo
    dt_inicial = pd.to_datetime(datetime.combine(data_inicial, time.min))
    dt_final = pd.to_datetime(datetime.combine(data_final, time.max))

    # --- Filtro de Atividades ---
    lista_nomes_atividades = sorted([info['nome'] for info in CAMPOS_DATA.values()])
    atividades_selecionadas = st.multiselect(
        "Filtrar Atividades/M√©tricas:",
        options=lista_nomes_atividades,
        default=lista_nomes_atividades, # Por padr√£o, mostrar todas
        key="filtro_atividades_prod"
    )

    if not atividades_selecionadas:
        st.warning("Selecione pelo menos uma atividade para visualizar os dados.")
        return # Impede a execu√ß√£o se nada for selecionado

    # Criar um dicion√°rio CAMPOS_DATA filtrado com base na sele√ß√£o
    CAMPOS_DATA_FILTRADO = {k: v for k, v in CAMPOS_DATA.items() if v['nome'] in atividades_selecionadas}
    colunas_data_filtradas = list(CAMPOS_DATA_FILTRADO.keys())

    # --- Processamento e An√°lise de Picos ---
    # Converter colunas de data para datetime no DataFrame de trabalho 'df', tratando erros
    # Usar colunas_data_necessarias (todas) para c√°lculo de picos e primeira data
    datas_validas_dict = {}
    for col in colunas_data_necessarias:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
            datas_validas_dict[col] = df[col].dropna() # Guarda datas v√°lidas por coluna
        else:
            df[col] = pd.NaT # Adiciona coluna vazia se n√£o existir
            datas_validas_dict[col] = pd.Series(dtype='datetime64[ns]') # S√©rie vazia

    # Calcular atividades totais por dia (usando as datas v√°lidas j√° convertidas)
    todas_as_datas_df = pd.concat(datas_validas_dict.values())
    atividades_por_dia = pd.Series(dtype=int) # Inicializa s√©rie vazia
    if not todas_as_datas_df.empty:
        # Agrupa por data (ignorando a hora) e conta as ocorr√™ncias
        atividades_por_dia = todas_as_datas_df.dt.normalize().value_counts().sort_index()

    # Calcular m√©dia e desvio padr√£o das atividades di√°rias GERAIS
    media_atividades_diarias = atividades_por_dia.mean()
    std_atividades_diarias = atividades_por_dia.std()
    limite_pico = media_atividades_diarias + 3 * std_atividades_diarias

    # Identificar dias com pico DENTRO do per√≠odo selecionado
    datas_periodo = pd.date_range(start=dt_inicial.normalize(), end=dt_final.normalize())
    dias_com_pico = atividades_por_dia[
        (atividades_por_dia.index.isin(datas_periodo)) &
        (atividades_por_dia > limite_pico)
    ]

    # Exibir aviso sobre picos, se houver
    if not dias_com_pico.empty:
        datas_pico_str = ", ".join([d.strftime('%d/%m/%Y') for d in dias_com_pico.index])
        st.warning(f"üìà **Aten√ß√£o:** Detectado volume de atividades significativamente alto nos dias: {datas_pico_str}. Isso pode indicar a√ß√µes autom√°ticas ou eventos incomuns.")
        # Op√ß√£o para remover dias com pico
        remover_picos = st.checkbox("Remover dias com pico da an√°lise?", key="remover_picos_prod")
    else:
        remover_picos = False # Garante que a vari√°vel exista mesmo sem picos

    # --- Filtragem de dados para o per√≠odo selecionado ---
    # Criar uma m√°scara para filtrar registros onde QUALQUER data RELEVANTE (SELECIONADA) cai no per√≠odo
    mascara_periodo = pd.Series(False, index=df.index)
    for col in colunas_data_filtradas: # Usa as colunas filtradas pela sele√ß√£o do usu√°rio
         if col in df.columns:
            # Assegura que a coluna foi convertida para datetime anteriormente
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                 mascara_periodo = mascara_periodo | ((df[col] >= dt_inicial) & (df[col] <= dt_final))

    df_filtrado_inicial = df[mascara_periodo].copy() # Filtro inicial pelo per√≠odo e atividades selecionadas

    # Aplicar exclus√£o de picos se solicitado e houver picos no per√≠odo
    if remover_picos and not dias_com_pico.empty:
        datas_pico_a_remover = dias_com_pico.index.normalize() # Datas normalizadas dos picos

        mascara_excluir_picos = pd.Series(True, index=df_filtrado_inicial.index) # Come√ßa permitindo tudo
        for col in colunas_data_necessarias: # Verifica picos em TODAS as colunas originais
             if col in df_filtrado_inicial.columns:
                  # Verifica se a coluna de data existe e n√£o est√° vazia antes de normalizar
                  if pd.api.types.is_datetime64_any_dtype(df_filtrado_inicial[col]) and not df_filtrado_inicial[col].isnull().all():
                     # Marca como False se a data (normalizada) da coluna estiver na lista de picos
                     mascara_excluir_picos = mascara_excluir_picos & (~df_filtrado_inicial[col].dt.normalize().isin(datas_pico_a_remover))

        df_filtrado = df_filtrado_inicial[mascara_excluir_picos].copy()
        st.info(f"Dias com pico ({datas_pico_str}) foram removidos da an√°lise.")
    else:
        df_filtrado = df_filtrado_inicial # Usa o df filtrado apenas pelo per√≠odo

    if df_filtrado.empty:
        # Mensagem ajustada para refletir poss√≠vel exclus√£o de picos
        if remover_picos and not dias_com_pico.empty:
             st.info(f"Nenhuma atividade encontrada entre {data_inicial.strftime('%d/%m/%Y')} e {data_final.strftime('%d/%m/%Y')} ap√≥s a remo√ß√£o dos dias com pico.")
        else:
             st.info(f"Nenhuma atividade encontrada entre {data_inicial.strftime('%d/%m/%Y')} e {data_final.strftime('%d/%m/%Y')}.")
        return

    # --- Exibir M√©tricas por Data (Cards Categorizados com Estilo) ---
    st.markdown("#### Atividades no Per√≠odo")

    metricas_por_categoria = {'SUCESSO': [], 'EM ANDAMENTO': [], 'FALHA': []}

    # Calcular contagem para cada campo SELECIONADO DENTRO do per√≠odo
    for col_tecnico, info in CAMPOS_DATA_FILTRADO.items(): # Itera sobre os campos filtrados
        if col_tecnico in df_filtrado.columns:
             # Conta IDs √∫nicos onde a data espec√≠fica cai no per√≠odo
            contagem = df_filtrado.loc[
                (df_filtrado[col_tecnico] >= dt_inicial) & (df_filtrado[col_tecnico] <= dt_final),
                'ID' # Assume que a coluna 'ID' existe e √© o identificador √∫nico
            ].nunique()
            metricas_por_categoria[info['categoria']].append({'label': info['nome'], 'value': contagem})
        # N√£o adiciona mais m√©tricas zeradas para campos n√£o selecionados
        # else:
            # Adiciona m√©trica zerada se a coluna estiver faltando (mas deveria existir se foi selecionada)
            # metricas_por_categoria[info['categoria']].append({'label': f"{info['nome']} (Campo Ausente)", 'value': 0})

    # Renderizar as m√©tricas nas colunas usando a estrutura de cards
    col_suc, col_and, col_fal = st.columns(3)

    with col_suc:
        # Usar a vari√°vel SCSS $success (compilada) - assumindo que a classe .category-title ou um estilo inline use essa vari√°vel
        # Temporariamente, vamos manter a cor aqui, mas idealmente viria do CSS compilado.
        # Se as vari√°veis SCSS estiverem corretamente mapeadas para classes CSS, poder√≠amos remover o style inline.
        st.markdown(f'<h5 class="category-title" style="color: var(--color-success, #22C55E); margin-bottom: 10px;">‚úÖ SUCESSO</h5>', unsafe_allow_html=True) # Cor atualizada para $success
        st.markdown('<div class="cards-grid">', unsafe_allow_html=True)
        for metrica in metricas_por_categoria['SUCESSO']:
            # Verificar se a m√©trica corresponde a uma atividade selecionada
            if metrica['label'] in atividades_selecionadas:
                 st.markdown(f"""
                 <div class="card-visao-geral card-visao-geral--sucesso fade-in">
                     <div class="card-visao-geral__title">{metrica['label']}</div>
                     <div class="card-visao-geral__metrics">
                         <span class="card-visao-geral__quantity">{metrica['value']:,}</span>
                     </div>
                 </div>
                 """, unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

    with col_and:
        # Usar a vari√°vel SCSS $warning (compilada)
        st.markdown(f'<h5 class="category-title" style="color: var(--color-warning, #F59E0B); margin-bottom: 10px;">‚è≥ EM ANDAMENTO</h5>', unsafe_allow_html=True) # Cor atualizada para $warning
        st.markdown('<div class="cards-grid">', unsafe_allow_html=True)
        for metrica in metricas_por_categoria['EM ANDAMENTO']:
             # Verificar se a m√©trica corresponde a uma atividade selecionada
             if metrica['label'] in atividades_selecionadas:
                st.markdown(f"""
                <div class="card-visao-geral card-visao-geral--em-andamento fade-in">
                    <div class="card-visao-geral__title">{metrica['label']}</div>
                    <div class="card-visao-geral__metrics">
                        <span class="card-visao-geral__quantity">{metrica['value']:,}</span>
                    </div>
                </div>
                """, unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

    with col_fal:
        # Usar a vari√°vel SCSS $danger (compilada)
        st.markdown(f'<h5 class="category-title" style="color: var(--color-danger, #EF4444); margin-bottom: 10px;">‚ùå FALHA</h5>', unsafe_allow_html=True) # Cor atualizada para $danger
        st.markdown('<div class="cards-grid">', unsafe_allow_html=True)
        for metrica in metricas_por_categoria['FALHA']:
             # Verificar se a m√©trica corresponde a uma atividade selecionada
             if metrica['label'] in atividades_selecionadas:
                st.markdown(f"""
                <div class="card-visao-geral card-visao-geral--falha fade-in">
                    <div class="card-visao-geral__title">{metrica['label']}</div>
                    <div class="card-visao-geral__metrics">
                        <span class="card-visao-geral__quantity">{metrica['value']:,}</span>
                    </div>
                </div>
                """, unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

    # --- Tabela de An√°lise por Respons√°vel ---
    st.markdown("#### An√°lise por Respons√°vel no Per√≠odo")

    # Preparar dados para a tabela pivot
    dados_pivot = []

    # Iterar sobre os registros filtrados pelo per√≠odo
    for index, row in df_filtrado.iterrows():
        id_processo = row['ID'] # Assume que 'ID' √© o identificador do processo

        # Verificar cada par Respons√°vel/Data mapeado
        for resp_col, data_col in MAPA_RESP_DATA.items():
            # Verifica se a DATA CORRESPONDENTE est√° entre as atividades SELECIONADAS
            if data_col in CAMPOS_DATA_FILTRADO:
                # Verificar se as colunas existem e se o respons√°vel n√£o √© nulo
                if resp_col in row.index and data_col in row.index and pd.notna(row[resp_col]):
                    # Verificar se a data correspondente est√° no per√≠odo
                    if pd.notna(row[data_col]) and dt_inicial <= row[data_col] <= dt_final:
                        responsavel_id = str(row[resp_col]) # Usar ID como string
                        # Usa o nome do CAMPOS_DATA_FILTRADO para garantir consist√™ncia
                        atividade_nome = CAMPOS_DATA_FILTRADO[data_col]['nome']

                        # Adicionar √† lista para pivotagem
                        dados_pivot.append({
                            'Responsavel_ID': responsavel_id,
                            'Atividade': atividade_nome,
                            'ID_Processo': id_processo
                        })

    if not dados_pivot:
        st.info("Nenhuma atividade de respons√°vel encontrada no per√≠odo selecionado para as colunas mapeadas.")
    else:
        df_pivot_base = pd.DataFrame(dados_pivot)

        # Verificar se df_pivot_base n√£o est√° vazio antes de pivotar
        if not df_pivot_base.empty:
            try:
                 # Criar tabela pivot: Respons√°vel vs Atividade, contando IDs √∫nicos de processo
                tabela_responsaveis = pd.pivot_table(
                    df_pivot_base,
                    values='ID_Processo',
                    index='Responsavel_ID',
                    columns='Atividade',
                    aggfunc='nunique', # Conta quantos processos √∫nicos cada respons√°vel fez por atividade
                    fill_value=0 # Preenche com 0 onde n√£o houve atividade
                )

                 # Ordenar colunas (atividades) pela ordem definida em CAMPOS_DATA_FILTRADO se poss√≠vel
                ordem_colunas = [info['nome'] for info in CAMPOS_DATA_FILTRADO.values() if info['nome'] in tabela_responsaveis.columns]
                # Garantir que colunas n√£o mapeadas (se houver) n√£o causem erro
                colunas_presentes_na_tabela = [col for col in ordem_colunas if col in tabela_responsaveis.columns]
                tabela_responsaveis = tabela_responsaveis[colunas_presentes_na_tabela]

                # Adicionar uma coluna TOTAL
                tabela_responsaveis['TOTAL GERAL'] = tabela_responsaveis.sum(axis=1)

                # Ordenar por TOTAL GERAL (descendente)
                tabela_responsaveis = tabela_responsaveis.sort_values('TOTAL GERAL', ascending=False)

                # --- Barra de Busca por Respons√°vel --- 
                busca_responsavel = st.text_input("Buscar Respons√°vel:", key="busca_resp_prod")

                tabela_filtrada = tabela_responsaveis
                if busca_responsavel:
                    # Filtrar o √≠ndice (Responsavel_ID) - case-insensitive
                    tabela_filtrada = tabela_responsaveis[
                        tabela_responsaveis.index.str.contains(busca_responsavel, case=False, na=False)
                    ]
                
                if tabela_filtrada.empty and busca_responsavel:
                    st.warning(f"Nenhum respons√°vel encontrado para o ID: '{busca_responsavel}'")
                elif not tabela_filtrada.empty:
                    st.dataframe(tabela_filtrada.style.format("{:,}"), use_container_width=True) # Formata n√∫meros
                    st.caption("A tabela mostra a contagem de processos √∫nicos tratados por cada respons√°vel (ID) em cada atividade, dentro do per√≠odo selecionado.")
                # N√£o mostrar nada se a busca inicial for vazia e n√£o houver resultados

            except Exception as e:
                 st.error(f"Ocorreu um erro ao gerar a tabela de respons√°veis: {str(e)}")
                 st.dataframe(df_pivot_base) # Mostra dados brutos se pivot falhar
        else:
             st.info("Nenhuma atividade de respons√°vel encontrada no per√≠odo selecionado ap√≥s o processamento.")

    # Adicionar aqui a l√≥gica e visualiza√ß√µes de produ√ß√£o
    # st.info("üöß Se√ß√£o em constru√ß√£o.") # Remover esta linha

    # Adicionar l√≥gica e visualiza√ß√µes de produ√ß√£o aqui 